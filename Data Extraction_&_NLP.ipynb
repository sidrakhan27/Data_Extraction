{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a762ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cff30c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0   123.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "1   321.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "2  2345.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "3  4321.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "4   432.0  https://insights.blackcoffer.com/rise-of-telem..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the excel file\n",
    "data = pd.read_excel('Input.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba36345",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directry = 'extracted_items'\n",
    "os.makedirs(output_directry, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef19a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_strcture_file = 'Output Data Structure.xlsx'\n",
    "output_dt = pd.read_excel(output_strcture_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f2031c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extracted text files\n",
    "extracted_text_directory = 'extracted_text'\n",
    "extracted_txts = []\n",
    "for filename in os.listdir(extracted_text_directory):\n",
    "    with open(os.path.join(extracted_text_directory, filename), 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        extracted_txts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56689d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_scores = []\n",
    "negative_scores = []\n",
    "polarity_scores = []\n",
    "subjectivity_scores = []\n",
    "avg_sentence_lengths = []\n",
    "percentage_complex_words = []\n",
    "fog_indexes = []\n",
    "avg_words_per_sentence = []\n",
    "complex_word_counts = []\n",
    "word_counts = []\n",
    "syllables_per_word = []\n",
    "personal_pronouns = []\n",
    "avg_word_lengths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3bc08fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and analyzed text for 123.0\n",
      "Extracted and analyzed text for 321.0\n",
      "Extracted and analyzed text for 2345.0\n",
      "Extracted and analyzed text for 4321.0\n",
      "Extracted and analyzed text for 432.0\n",
      "Extracted and analyzed text for 2893.8\n",
      "Extracted and analyzed text for 3355.6\n",
      "Extracted and analyzed text for 3817.4\n",
      "Extracted and analyzed text for 4279.2\n",
      "Extracted and analyzed text for 4741.0\n",
      "Extracted and analyzed text for 5202.8\n",
      "Extracted and analyzed text for 5664.6\n",
      "Extracted and analyzed text for 6126.4\n",
      "Extracted and analyzed text for 6588.2\n",
      "Extracted and analyzed text for 7050.0\n",
      "Extracted and analyzed text for 7511.8\n",
      "Extracted and analyzed text for 7973.6\n",
      "Extracted and analyzed text for 8435.4\n",
      "Extracted and analyzed text for 8897.2\n",
      "Extracted and analyzed text for 9359.0\n",
      "Extracted and analyzed text for 9820.8\n",
      "Extracted and analyzed text for 10282.6\n",
      "Extracted and analyzed text for 10744.4\n",
      "Extracted and analyzed text for 11206.2\n",
      "Failed to access https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Extracted and analyzed text for 12129.8\n",
      "Extracted and analyzed text for 12591.6\n",
      "Extracted and analyzed text for 13053.4\n",
      "Extracted and analyzed text for 13515.2\n",
      "Extracted and analyzed text for 13977.0\n",
      "Extracted and analyzed text for 14438.8\n",
      "Extracted and analyzed text for 14900.6\n",
      "Extracted and analyzed text for 15362.4\n",
      "Extracted and analyzed text for 15824.2\n",
      "Extracted and analyzed text for 16286.0\n",
      "Extracted and analyzed text for 16747.8\n",
      "Extracted and analyzed text for 17209.6\n",
      "Failed to access https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Extracted and analyzed text for 18133.2\n",
      "Extracted and analyzed text for 18595.0\n",
      "Extracted and analyzed text for 19056.8\n",
      "Extracted and analyzed text for 19518.6\n",
      "Extracted and analyzed text for 19980.4\n",
      "Extracted and analyzed text for 20442.2\n",
      "Extracted and analyzed text for 20904.0\n",
      "Extracted and analyzed text for 21365.8\n",
      "Extracted and analyzed text for 21827.6\n",
      "Extracted and analyzed text for 22289.4\n",
      "Extracted and analyzed text for 22751.2\n",
      "Extracted and analyzed text for 23213.0\n",
      "Extracted and analyzed text for 23674.8\n",
      "Extracted and analyzed text for 24136.6\n",
      "Extracted and analyzed text for 24598.4\n",
      "Extracted and analyzed text for 25060.2\n",
      "Extracted and analyzed text for 25522.0\n",
      "Extracted and analyzed text for 25983.8\n",
      "Extracted and analyzed text for 26445.6\n",
      "Extracted and analyzed text for 26907.4\n",
      "Extracted and analyzed text for 27369.2\n",
      "Extracted and analyzed text for 27831.0\n",
      "Extracted and analyzed text for 28292.8\n",
      "Extracted and analyzed text for 28754.6\n",
      "Extracted and analyzed text for 29216.4\n",
      "Extracted and analyzed text for 29678.2\n",
      "Extracted and analyzed text for 30140.0\n",
      "Extracted and analyzed text for 30601.8\n",
      "Extracted and analyzed text for 31063.6\n",
      "Extracted and analyzed text for 31525.4\n",
      "Extracted and analyzed text for 31987.2\n",
      "Extracted and analyzed text for 32449.0\n",
      "Extracted and analyzed text for 32910.8\n",
      "Extracted and analyzed text for 33372.6\n",
      "Extracted and analyzed text for 33834.4\n",
      "Extracted and analyzed text for 34296.2\n",
      "Extracted and analyzed text for 34758.0\n",
      "Extracted and analyzed text for 35219.8\n",
      "Extracted and analyzed text for 35681.6\n",
      "Extracted and analyzed text for 36143.4\n",
      "Extracted and analyzed text for 36605.2\n",
      "Extracted and analyzed text for 37067.0\n",
      "Extracted and analyzed text for 37528.8\n",
      "Extracted and analyzed text for 37990.6\n",
      "Extracted and analyzed text for 38452.4\n",
      "Extracted and analyzed text for 38914.2\n",
      "Extracted and analyzed text for 39376.0\n",
      "Extracted and analyzed text for 39837.8\n",
      "Extracted and analyzed text for 40299.6\n",
      "Extracted and analyzed text for 40761.4\n",
      "Extracted and analyzed text for 41223.2\n",
      "Extracted and analyzed text for 41685.0\n",
      "Extracted and analyzed text for 42146.8\n",
      "Extracted and analyzed text for 42608.6\n",
      "Extracted and analyzed text for 43070.4\n",
      "Extracted and analyzed text for 43532.2\n",
      "Extracted and analyzed text for 43994.0\n",
      "Extracted and analyzed text for 44455.8\n",
      "Extracted and analyzed text for 44917.6\n",
      "Extracted and analyzed text for 45379.4\n",
      "Extracted and analyzed text for 45841.2\n",
      "Extracted and analyzed text for 46303.0\n",
      "Extracted and analyzed text for 46764.8\n",
      "Extracted and analyzed text for 47226.6\n",
      "Extracted and analyzed text for 47688.4\n",
      "Extracted and analyzed text for 48150.2\n",
      "Extracted and analyzed text for 48612.0\n",
      "Extracted and analyzed text for 49073.8\n",
      "Extracted and analyzed text for 49535.6\n",
      "Extracted and analyzed text for 49997.4\n",
      "Extracted and analyzed text for 50459.2\n",
      "Extracted and analyzed text for 50921.0\n",
      "Extracted and analyzed text for 51382.8\n",
      "Extracted and analyzed text for 51844.6\n",
      "Extracted and analyzed text for 52306.4\n",
      "Extracted and analyzed text for 52768.2\n"
     ]
    }
   ],
   "source": [
    "#loop through the rows in the DataFrame\n",
    "for index, row in data.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    article_url = row['URL']\n",
    "    \n",
    "    # Access the URL and extract article text\n",
    "    response = requests.get(article_url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find article title\n",
    "        article_title = soup.find('title').get_text()\n",
    "        \n",
    "        # Find and extract article text\n",
    "        article_text = ''\n",
    "        article_contents = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        for content in article_contents:\n",
    "            article_text += content.get_text() + '\\n'\n",
    "        \n",
    "        # Save the extracted text to a file\n",
    "        filename = os.path.join(output_directry, f'{url_id}.txt')\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(article_title + '\\n')\n",
    "            file.write(article_text)\n",
    "        \n",
    "        # Text Analysis for each extracted text\n",
    "        blob = TextBlob(article_text)\n",
    "        \n",
    "        # Sentiment Analysis\n",
    "        sentiment = blob.sentiment\n",
    "        polarity_scores.append(sentiment.polarity)\n",
    "        subjectivity_scores.append(sentiment.subjectivity)\n",
    "    \n",
    "        # Calculate positive and negative scores based on polarity\n",
    "        positive_score = max(sentiment.polarity, 0)  # Assigning polarity as positive score if positive\n",
    "        negative_score = max(-sentiment.polarity, 0)  # Assigning negative of polarity as negative score if negative\n",
    "    \n",
    "        positive_scores.append(positive_score)\n",
    "        negative_scores.append(negative_score)\n",
    "        # Tokenization\n",
    "        sentences = sent_tokenize(article_text)\n",
    "        words = word_tokenize(article_text)\n",
    "        \n",
    "        # Average Sentence Length\n",
    "        avg_sentence_lengths.append(len(words) / len(sentences))\n",
    "        \n",
    "        # Percentage of Complex Words\n",
    "        stopwords_set = set(stopwords.words('english'))\n",
    "        complex_word_count = sum(1 for word in words if word.lower() not in stopwords_set)\n",
    "        percentage_complex = (complex_word_count / len(words)) * 100\n",
    "        percentage_complex_words.append(percentage_complex)\n",
    "        \n",
    "        # FOG Index\n",
    "        fog_index = 0.4 * (avg_sentence_lengths[-1] + percentage_complex)\n",
    "        fog_indexes.append(fog_index)\n",
    "        \n",
    "        # Average Number of Words per Sentence\n",
    "        avg_words_per_sentence.append(len(words) / len(sentences))\n",
    "        \n",
    "        # Complex Word Count\n",
    "        complex_word_counts.append(complex_word_count)\n",
    "        \n",
    "        # Word Count\n",
    "        word_counts.append(len(words))\n",
    "        \n",
    "        # Syllables per Word (approximate)\n",
    "        syllables = sum(len(word) / 3 for word in words)  # Approximation\n",
    "        syllables_per_word.append(syllables / len(words))\n",
    "        \n",
    "        # Personal Pronouns (counting first and second person pronouns)\n",
    "        personal_pronoun_count = sum(1 for word in words if word.lower() in ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'])\n",
    "        personal_pronouns.append(personal_pronoun_count)\n",
    "        \n",
    "        # Average Word Length\n",
    "        \n",
    "        avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "        avg_word_lengths.append(avg_word_length)\n",
    "        \n",
    "        print(f'Extracted and analyzed text for {url_id}')\n",
    "    else:\n",
    "        print(f'Failed to access {article_url}')\n",
    "        \n",
    "        # Handle missing data by assigning default values for computed variables\n",
    "        \n",
    "        positive_scores.append(0)\n",
    "        negative_scores.append(0)\n",
    "        polarity_scores.append(0)\n",
    "        subjectivity_scores.append(0)\n",
    "        avg_sentence_lengths.append(0)\n",
    "        percentage_complex_words.append(0)\n",
    "        fog_indexes.append(0)\n",
    "        avg_words_per_sentence.append(0)\n",
    "        complex_word_counts.append(0)\n",
    "        word_counts.append(0)\n",
    "        syllables_per_word.append(0)\n",
    "        personal_pronouns.append(0)\n",
    "        avg_word_lengths.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4af9e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the computed variables\n",
    "output_dt = pd.DataFrame()\n",
    "output_dt['URL_ID'] = data['URL_ID']\n",
    "output_dt['URL'] = data['URL']\n",
    "output_dt['POSITIVE SCORE'] = positive_scores\n",
    "output_dt['NEGATIVE SCORE'] = negative_scores\n",
    "output_dt['POLARITY SCORE'] = polarity_scores\n",
    "output_dt['SUBJECTIVITY SCORE'] = subjectivity_scores\n",
    "output_dt['AVG SENTENCE LENGTH'] = avg_sentence_lengths\n",
    "output_dt['PERCENTAGE OF COMPLEX WORDS'] = percentage_complex_words\n",
    "output_dt['FOG INDEX'] = fog_indexes\n",
    "output_dt['AVG NUMBER OF WORDS PER SENTENCE'] = avg_words_per_sentence\n",
    "output_dt['COMPLEX WORD COUNT'] = complex_word_counts\n",
    "output_dt['WORD COUNT'] = word_counts\n",
    "output_dt['SYLLABLE PER WORD'] = syllables_per_word\n",
    "output_dt['PERSONAL PRONOUNS'] = personal_pronouns\n",
    "output_dt['AVG WORD LENGTH'] = avg_word_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "543dbbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "output_result_file = 'output_results.xlsx'\n",
    "output_dt.to_excel(output_result_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb733d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
